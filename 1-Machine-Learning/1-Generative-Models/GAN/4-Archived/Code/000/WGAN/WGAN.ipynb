{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Flatten, Reshape, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras import backend\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.constraints import Constraint\n",
    "\n",
    "class ClipConstraint(tf.keras.constraints.Constraint):\n",
    "    # set clip value when initialized\n",
    "    def __init__(self, clip_value):\n",
    "        self.clip_value = clip_value\n",
    "\n",
    "    # clip model weights to hypercube\n",
    "    def __call__(self, weights):\n",
    "        return backend.clip(weights, -self.clip_value, self.clip_value)\n",
    "\n",
    "    # get the config\n",
    "    def get_config(self):\n",
    "        return {'clip_value': self.clip_value}\n",
    "\n",
    "def generator_model():\n",
    "    \n",
    "    init = RandomNormal(stddev=0.02) # weight initialization\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(7 * 7 * 128, kernel_initializer=init, input_dim=100))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    " \n",
    "    # upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4, 4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    " \n",
    "    # upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4, 4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    " \n",
    "    # output 28x28x1\n",
    "    model.add(Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=init))\n",
    "\n",
    "    return model\n",
    "\n",
    "def discriminator_model():\n",
    "    \n",
    "    init = RandomNormal(stddev=0.02) # weight initialization  \n",
    "    const = ClipConstraint(0.01) # weight constraint\n",
    "\n",
    "    model = Sequential()\n",
    " \n",
    "    # downsample to 14x14\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init, kernel_constraint=const, input_shape=(28, 28, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    " \n",
    "    # downsample to 7x7\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init, kernel_constraint=const))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    " \n",
    "    # scoring, linear activation\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    " \n",
    "    return model\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):  \n",
    "    return -tf.reduce_mean(real_output) + tf.reduce_mean(fake_output)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "\n",
    "generator_optimizer = RMSprop(learning_rate=0.00005)\n",
    "discriminator_optimizer = RMSprop(learning_rate=0.00005)\n",
    "\n",
    "################################################################\n",
    "\n",
    "@tf.function\n",
    "def train_discriminator(images):\n",
    "\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "\n",
    "        noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "@tf.function\n",
    "def train_generator():\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "\n",
    "        noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    for _ in range(n_critic):\n",
    "\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            generated_images = generator(noise, training=True)\n",
    "            real_output = discriminator(images, training=True)\n",
    "            fake_output = discriminator(generated_images, training=True)\n",
    "   \n",
    "            disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "################################\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:   \n",
    "            # for _ in range(n_critic):\n",
    "            #     train_discriminator(image_batch)\n",
    "            # train_generator()\n",
    "\n",
    "            train_step(image_batch)\n",
    "    \n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # 最后一个 epoch 结束后生成图片\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epochs, seed)\n",
    "\n",
    "generator = generator_model()\n",
    "discriminator = discriminator_model()\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "n_critic=5\n",
    "\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images[train_labels == 0] # select a given class\n",
    "train_images = np.expand_dims(train_images, axis=-1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "\n",
    "BUFFER_SIZE = train_images.shape[0] # 数据集大小\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# 批量化和打乱数据\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "\n",
    "train(train_dataset, EPOCHS)"
   ]
  }
 ]
}