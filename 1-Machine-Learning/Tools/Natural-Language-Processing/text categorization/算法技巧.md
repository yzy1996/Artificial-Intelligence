# Tweet Sentiment classifier

## 1. Introduction

Twitter is a popular microblogging service and it is also the name of a website. In comparison tweets are the messages that are sent over twitter, with no more than 140 characters. Each tweet can be classified as “positive”, “neutral” and “negative”. For an example, a tweet like “http://t.co/QV4m1Un9 Forget the phone.. Nice UI. Liking the Scroll Feature #android #google #nexus” and its sentiment is “positive”. Here I introduce a series of ideas and methods to automatically classify the sentiment. 

Big challenges can be faced in tweet sentiment classifier: (i) neutral tweets are more common than positive and negative ones so its an imbalanced label problem. This is different from other sentiment classifier domains(e.g. product reviews), which tend to be predominantly positive or negative; (ii) tweets are very short and often show limited sentiment cues.

There are many traditional classifiers like Naive Bayes, Random Forest, and Support Vector Machines to solve such problems. Not only that, we can also use some methods of neural networks which are more complicated. 



## 2. Main work

### a. Data analysis

From the introduction, we have known the maximum of characters is 140 and its a very important value. We can limit our feature map to a limited number rather than a dynamic number. (i) After we get the dataset, we can draw the hist of class distribution. in most cases, “neutral” is domain to “positive” and “negative”. (ii) we can count all the words that appear in the dataset.

### b. Data preprocess

It's important to do the data preprocessing because there are some irrelevant and redundant information. These noises will take considerable amount of processing time and make it difficlut to train a classifier. All in all, data preprocessing will help you get a better result.

In the train and test set, there are many URLS and some names followed by #@. Besides stop words, links, punctuation are also need to be removed. These words will not do help to our result so we need to delete them. It’s convenient to use regular expression.

Here is a trick that we can use the opinion lexicon which is a list including negative and positive words. Examples of positive opinion words are beautiful, wonderful, good, and amazing and
examples of negative opinion words are bad, poor, and terrible.

### c. Tweets representation

There are three strategies to represent the tweets: (i) bag-of-words. Tweets are represented by a table in which the columns represent the terms in the tweets. Therefore, we get a collection of tweets which contains n tweets and m terms. Each tweet is represented as $(a_{i1}, a_{i2}, \dots, a_{im})$,where $a_{ij}$ is the frequency of term $t_j$ in the $tweet_i$.  (ii) feature hashing. This method can reduce the number of features provided as input to a learning algorithm. The original high-dimensional space is reduced by hashing the features into a lower-dimensional space. (iii) word-vector. It is a row of real-valued numbers where each point captures a dimension of the word's meaning and where semantically similar words have similar vectors. We can say that word vectors represent the meaning of a word and it’s different to the other strategies before.

However, the simple bag-of-words use term frequency to represent the words and a better way is adding inverse document frequency. This TD-IDF will help us get the real importance.

### d. Classifier training

It's so happy that sklearn has offered many existing classifier and we can use them directly. I have delved into Naive Bayes model and support vector machine model. After we train the classifier, we should evaluate the quality of this classifier. It's better for us to evaluate the result on the test set. Unfortunately, we can only submit our result two times everyday. So I choose the method of cross_val_score. Then, the parameters will determine the effect of the classifier. We can use GridSearchCV to help us choose best parameters automatically.

### e. Model stacking

We can find that each classifier has a different result on the test set. It will be better to combine these classifier to get a combined classifier. The easiest way is to use the method of voting. For an example, if we have trained three classifiers and we get the result of 2 “positive” 1 “negative”, then we will vote to “positive”. Unfortunately, it’s okay to choose randomly if we get three different results.

  

 ## 3. Conclusion

I have tried many ways to improve the performance. I consulted the data sheet over and over and read several related papers. What makes me desperate is that many ways do not improve the result, but the simplest model is most effective. Here I will explain in theory what I think should be effective.

The first is data preprocessing, URLs, numbers, irrelevant words, etc. are not helpful because they have no Sentiment meaning. They can be regarded as noise, so we must find a way to remove them. The second is feature extraction. In the bag-of-words model, tf-idf performs better than tf, which can be analyzed in principle. It is also better to use a word vector model on a larger data. Finally is the classifier training. The first thing we should do is to use as many classifiers as possible to compare the effects. Then we try to use as many parameters as possible to compare the effects. So that we have an optimal classifier with the best parameters. Similarly, for a large data set, neural networks can also be useful for better results. When encountering multiple classifiers, the effects of the classifier are not inferior, we can consider model stacking to make the classifier achieve better results.



























