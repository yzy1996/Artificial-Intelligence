在文本分类中用到的技巧



HashingVectorizer

KNeighborsClassifierk 最近邻分类功能

N-gram

n-gram模型用于评估语句是否合理



解决词袋模型局限性

训练集中没有出现的词会被排除掉



上图为sklearn提供的关于模型选择的参考图。该图的源网页为（[https://scikit-learn.org/stable/tutorial/machine_learning_map/](https://link.zhihu.com/?target=https%3A//scikit-learn.org/stable/tutorial/machine_learning_map/)）。图中左上为分类算法，左下为聚类算法。在聚类算法中，GMM、VBGMM为语音处理算法。MeanShift为图像处理算法。因此应用于文本的聚类算法有**KMeans**（数据量小于1万）、**MiniBatchKMeans**（数据量大于1万）、**Spectral Clustering**（数据量小于1万），它们都是用来解决已知簇数量的算法，对于未知簇数量的情况，可以采用增量聚类算法比如**single-pass**。在分类算法中当数据量小于10万时，使用分类器**LinearSVC**或者**NaiveBayes**。当数据量大于10万时采用随机梯度下降线性分类器**SGD Classifier**。







https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py

https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf

https://scikit-learn.org/stable/modules/feature_extraction.html?highlight=gram





http [a-zA-z]+://[^\s^,]*





统计仅单词



|                | 训练集词汇量 | 测试集词汇量 | 相差 |
| :------------: | :----------: | :----------: | :--: |
| 原词库提取单词 |     3739     |     2382     | 638  |
| 原词库不取单词 |     7002     |     3925     | 2260 |
|                |              |              |      |





[https://github.com/srinivas9804/Machine-Leaning/blob/master/Assignment%201/Assignment1.ipynb](https://github.com/srinivas9804/Machine-Leaning/blob/master/Assignment 1/Assignment1.ipynb)











## Outline

- 1. Data analysis

  - hist of class distribution
  - words of the feature_names

- 2. Data preprocess
  
  - delete 
- word lemmatize
  
- 3. Feature extraction 
  
  - BOW
  - wordvector
  - 
  
- 4. Classifier training
  
     Model evaluation
  
  - sklearn
  - 
  - 
  - NN
    - LSTM
  
- 5. model Stacking

  - voting
  - mean

- 6. model retrain

  - ss

- 7. result evaluate

  

# 1. Data analysis

The first step is to analyze the data you get. Take the training set as an example:

Here we use the histogram to see the distribution of the data

# 2. Data pre-processing
It' important to do the data pre-processing because there are some irrelevant and redundant information. These noises will take considerable amount of processing time and make it difficlut to train a classifier. All in all, data preprocessing will help you get a better result.

# 3. Feature extraction 

We are already familiar with the bag of words model. ref

3.1 BOW

3.2 





我尝试了很多对提升性能有帮助的方法，一遍又一遍查阅数据手册，读了几篇相关的论文，让我很绝望的是很多方法对我的提升并不大，反而是最简单的模型有用更好的结果。在这里我将从理论上阐述我觉得应该有效的方法。

首先是数据预处理，网址，数字，无关紧要的词语等都是没有帮助的，因为他们不具有情感的含义，可以被看成是噪声，因此要想办法将他们去掉。可以使用的方法有：1.使用re+正则表达式直接从训练测试集中删除 2.在vecrorizer中添加参数（analyzer,  stop_words, token_pattern）3.统一词的形式，例如 come came 其实是一个含义，所以就要统一 4.确保他是单词，删除错误的单词 后面两种方法都可以使用一些库工具



其次特征提取，在词袋模型中，tf-idf比tf效果好，这是从原理上可以分析出来的。如果在更大的样本上，使用词向量模型也会更好



分类器训练，首先是使用尽可能多的分类器来比较效果，其次是尝试尽可能多的参数来比较效果，这样我们就得到了一个使用最佳参数的最佳分类器，同样，对于一个更大的数据集，神经网络也会有用更好的效果。当遇见多个分类器效果不分上下时，我们可以考虑模型融合，以一种少数服从多数的方式来使得分类器达到更好的效果



I have tried many ways to improve the performance. I consulted the data sheet over and over and read several related papers. What makes me desperate is that many ways do not improve the result, but the simplest model is most effective. Here I will explain in theory what I think should be effective.

1. The first is data preprocessing, URLs, numbers, irrelevant words, etc. are not helpful because they have no Sentiment meaning. They can be regarded as noise, so we must find a way to remove them. The methods that can be used: 

   Use `re`  and `regular expressions` to directly delete from the training and test set 

   Add `parameters` (analyzer, stop_words, token_pattern) in the vecrorizer 

   Unify the form of words, such as “come came” is actually a word

   Make sure it is a word and delete the wrong word

   The latter two methods can use some library tools like `NLTK`



2. The second is feature extraction. In the bag-of-words model, tf-idf performs better than tf, which can be analyzed in principle. It is also better to use a word vector model on a larger data.



3. Finally is the classifier training. The first thing we should do is to use as many classifiers as possible to compare the effects. Then we try to use as many parameters as possible to compare the effects. So that we have an optimal classifier with the best parameters. Similarly, for a large data set, neural networks can also be useful for better results. When encountering multiple classifiers, the effects of the classifier are not inferior, we can consider model stacking to make the classifier achieve better results.



























