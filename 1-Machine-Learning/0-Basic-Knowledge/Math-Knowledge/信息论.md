# Information Theory

> all $\log$ is base 2

## 1. Definition

the **informational value** of an event $x$ with probability $p(x)$ is:
$$
\mathrm{I}(x) = -\log(p(x))
$$
the **entropy** $Η$ of a discrete random variable $X$ with possible values $\left\{x_{1}, \ldots, x_{n}\right\}$ is:
$$
H(X)=\mathrm{E}[\mathrm{I}(X)]=\mathrm{E}[-\log (\mathrm{P}(X))]=-\sum_{i=1}^n p(x_i)\log p(x_i)
$$
the **relative entropy** (also called **Kullback–Leibler divergence**) of discrete probability distribution $p$ and $q$ defined on the same probability space $\mathcal{X}$ is:
$$
D_{\mathrm{KL}}(p \| q)=\sum_{x \in \mathcal{X}} p(x) \log \left(\frac{p(x)}{q(x)}\right)
$$
the **cross entropy** of discrete probability distribution $p$ and $q$ with the same support $\mathcal{X}$ is:
$$
H(p, q) = -\mathrm{E}_{p}[\log q] = -\sum_{x \in \mathcal{X}} p(x) \log q(x)
$$
the relationship between relative entropy and cross entropy is:
$$
H(p, q)=H(p)+D_{\mathrm{KL}}(p \| q)
$$

doubuguai

the **information gain** is a synonym for KL divergence,

in the context of decision trees, it's the conditional expected value of KLD of the univariate probability distribution of one variable from the conditional distribution of this variable given the other one 























- a set of training examples: $T = \{(\mathbf{x}^1, y^1), (\mathbf{x}^2, y^2), \dots, (\mathbf{x}^n, y^n)\}$, and $y^i \in Y$

- each of the form: $(\mathbf{x}^i, y^i) = (x^i_1, x^i_2, \dots, x^i_m, y^i)$

- where $x^i_j$ is the value of the $j^{th}$ attribute (named $a_j$) of $\mathbf{x}^i$, and $x_j^i \in {V_j} = \{v_1, \dots, v_k\}$ ($k$ depends on $j$)

**information entropy** of data $T$: 
$$
H(T) = -\sum_{y \in Y} p(y) \log p(y)
$$

**conditional information entropy** of $T$ given the attribute $a_j$: 

> predefine a set of $T$ for which attribute $a_j$ is equal to $v$: $S_j(v) = \{(\mathbf{x},y) \in T | x_j = v\} \leftarrow$

$$
H(T|a_j) = \sum_{v \in V_j} p_j(v) H(S_j(v)) = \sum_{v \in V_j} \frac{|S_j(v)|}{|T|} H(S_j(v))
$$

**information gain**: 
$$
IG(T, a_j) = H(T) - H(T|a_j)
$$

**intrinsic value information (split information)**:
$$
IV(T, a_j) = -\sum_{v \in V_j} \frac{|S_j(v)|}{|T|} \log \frac{|S_j(v)|}{|T|}
$$
**information gain ratio**: 
$$
IGR(T, a_j) = \frac{IG(T, a_j)}{IV(T, a_j)}
$$

|                | attribute1 ($a_1$) | attribute2 ($a_2$) | class label |       |
| :------------: | :----------------: | :----------------: | :---------: | :---: |
| $\mathbf{x}^1$ |        True        |        True        |      0      | $y^1$ |
| $\mathbf{x}^2$ |        True        |       False        |      1      | $y^2$ |
| $\mathbf{x}^3$ |       False        |        True        |      1      | $y^3$ |
| $\mathbf{x}^4$ |       False        |        True        |      0      | $y^4$ |
| $\mathbf{x}^5$ |       False        |       False        |      1      | $y^5$ |

$$
\begin{aligned}
&H(T) = -\frac{3}{5} \log (\frac{3}{5}) - \frac{2}{5} \log (\frac{2}{5}) = 0.97 \\
&H(T|a_1) = \frac{2}{5} \left(-\frac{1}{2} \log (\frac{1}{2})-\frac{1}{2} \log (\frac{1}{2})\right) + \frac{3}{5} \left(-\frac{2}{3} \log (\frac{2}{3})-\frac{1}{3} \log (\frac{1}{3})\right) = 0.95 \\
&IG(T,a_1) = H(T) - H(T|a_1) = 0.02 \\
&IV(T,a_1) = -\frac{2}{5} \log (\frac{2}{5}) - \frac{3}{5} \log (\frac{3}{5}) = 0.97 \\
&IGR(T,a_1) = \frac{IG(T,a_1)}{IV(T,a_1)} = 0.02
\end{aligned}
$$









## 3. Recommend

https://blog.csdn.net/tsyccnh/article/details/79163834

https://blog.csdn.net/rtygbwwwerr/article/details/50778098



## 4. 

交叉熵为什么越小越好

什么地方使用交叉熵，和MSE比较有什么好处呢

