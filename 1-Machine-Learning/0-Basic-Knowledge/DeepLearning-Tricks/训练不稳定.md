# 训练不稳定



## Loss related 

> Loss suddenly increased using Adam optimizer



AMSGrad 

gradient clipping and batch normalization



One possible reason could be the numerical instability of some weights or gradients.

For example, some weights or gradients might become too small, so when you do the calculations with them, it gives incorrect ("exploding") results. Same could happen if they become too large.



One possible cause is a high learning rate. High values of this hyperparameter usually cause updates that are too drastic, and therefore divergence from the optimum.