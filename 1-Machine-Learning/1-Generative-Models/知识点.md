## Autoencoders

Neural networks are often characterized by the data type of their input and output. Many image classifiers take images as inputs, and output a probability distribution of the classes. However, Autoencoders (AE) are a family of neural networks for which the input is the same as the output. They work by compressing the input into a latent-space representation, and then reconstructing the output from this representation.

The reconstruction of the input image is often blurry and of lower quality. This is a consequence of the compression during which we have lost some information.

## Latent space

![img](https://hackernoon.com/hn-images/1*op0VO_QK4vMtCnXtmigDhA.png)

In an autoencoder, the encoder brings the data from a high dimensional input to a bottleneck layer, where the number of neurons is the smallest. Then, the decoder takes this encoded input and converts it back to the original input shape. The latent space is the space in which the data lies in the bottleneck layer.

The latent space contains a compressed representation of the image, which is the only information the decoder is allowed to use to try to reconstruct the input as faithfully as possible. To perform well, the network has to learn to extract the most relevant features in the bottleneck.

## GAN

The generative model in the GAN architecture learns to map points in the latent space to generated images.

A series of points can be created on a linear path between two points in the latent space, such as two generated images. These points can be used to generate a series of images that show a transition between the two generated images.