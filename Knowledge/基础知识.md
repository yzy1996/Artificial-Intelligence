# 基础知识

## 概率函数

概率质量函数：Probability mass function (PMF)，分布律

概率密度函数：Probability density function (PDF)

累积分布函数：Cumulative distribution function (CDF)



**PMF**是针对离散随机变量而言的，是随机变量在各特定取值上的概率

> 例如抛骰子，每一面朝上的概率都是1/6，那么表示出的PMF就是：
>
> $f_{X}(x)=\left\{\begin{array}{l}{\frac{1}{6} \text { if } x \in\{1,2,3,4,5,6\}} \\ {0 \text { if } x \notin\{1,2,3,4,5,6\}}\end{array}\right.$



**PDF**是针对连续随机变量而言的，因为连续所以我们无法描述变量落在某一点上的概率，只能说落在某一区间上的概率，官方描述为：在某个确定的取值点附近的可能性的函数

我们经常看到的均匀分布，高斯分布，说的就是概率密度函数。

> 例如均匀分布，它的概率密度函数是：
>
> $f(x)=\left\{\begin{array}{ll}{\frac{1}{b-a}} & {\text { for } a \leq x \leq b} \\ {0} & {\text { elsewhere }}\end{array}\right.$
>
> ![1567845043390](https://img-blog.csdnimg.cn/20190920095635829.png)
>
> 如果我们说落在每一点上的概率是$\frac{1}{b-a}$，那么岂不是（b-a）个点就使得总概率为1了吗？所以并不是这样，应该是PDF函数的积分才是函数，也即PDF函数图像的面积。



**CDF**是是概率密度函数的积分

$F(x)=P(X<=x)$



---



## 期望和方差

the **expectation** of $f(x)$ , denoted by $\mathbb{E}[f]$ : the average value of some function $f(x)$ under a probability distribution $p(x)$.

for a discrete distribution
$$
\mathbb{E}[f]=\sum_{x} p(x) f(x)
$$
for a continuous distribution
$$
\mathbb{E}[f]=\int p(x) f(x) \mathrm{d} x
$$

---



## Distribution 分布

### 1.Gaussian

 $x \sim \mathcal{N}(\mu, \sigma)$
$$
p(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
$$
$E[x]=\mu, D[x]=\sigma^2$



The multivariate Gaussian distribution 多元高斯分布

 $\boldsymbol{x},\boldsymbol{\mu}$ are d-dimensional,  $\boldsymbol{x} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$
$$
p(\boldsymbol{x})==\frac{1}{(2 \pi)^{d / 2}|\boldsymbol{\Sigma}|^{1 / 2}} \exp \left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\}
$$
$E[\boldsymbol{x}]=\boldsymbol{\mu}, D[\boldsymbol{x}]=\boldsymbol{\Sigma}$ 

$\frac{\partial p(\boldsymbol{x})}{\partial \boldsymbol{x}}=-p(\boldsymbol{x}) \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})$

### 2.Poisson

$$
p(x=k | \lambda)=\frac{1}{k !} e^{-\lambda} \lambda^{k}
$$

$E[X] = D[X] = \lambda$



### 3.Exponential

$$
p(x | \lambda)=\lambda e^{-\lambda x}
$$

$E[x]=\frac{1}{\lambda}, D[x]=\frac{1}{\lambda^2}$





###  二项分布、泊松分布、正态分布关系

1. 泊松分布，二项分布都是离散分布；正态分布是连续分布。

2. 二项分布什么时候趋近于泊松分布，什么时候趋近于正态分布？

   这么说吧：二项分布有两个参数，一个 n 表示试验次数，一个 p 表示一次试验成功概率。
   现在考虑一个二项分布，其中试验次数 n 无限增加，而 p 是 n 的函数。
   如果 np 存在有限极限 λ，则这列二项分布就趋于参数为 λ 的 泊松分布。反之，如果 np 趋于无限大（如 p 是一个定值），则根据德莫佛-拉普拉斯(De’Moivre-Laplace)中心极限定理，这列二项分布将趋近于正态分布。

   也就是说泊松分布和正态分布都来自于二项分布

3. 实际运用中当 n 很大时一般都用正态分布来近似计算二项分布，但是如果同时 np 又比较小（比起n来说很小），那么用泊松分布近似计算更简单些，毕竟泊松分布跟二项分布一样都是离散型分布。



### 理解记忆多维高斯分布

我们先看一维的正态分布：
$$
N\left(x| \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right)
$$
大家都没问题哈！

如果假设变量是相互独立的，那么可以得到变量的协方差矩阵的是对角矩阵
$$
\Sigma = \left[\begin{array}{cccc}{\sigma^2} & 0 & {\dots} & 0 \\ 0 & {\sigma^2} & {\dots} & 0 \\ {\dots} & {\dots} & {\dots} & {\dots} \\ 0 & 0 & {\dots} & {\sigma^2}\end{array}\right]
$$
x 是 d 维的向量， $\Sigma$ 是 x 的协方差矩阵
$$
N(x | u, \Sigma)=\frac{1}{(2 \pi)^{d/2}{|\Sigma|^{1 / 2}}} \exp \left[-\frac{1}{2}(x- u)^{T} \Sigma^{-1}(x-u)\right]
$$



---



## 样本估计

we have the **population mean** $\mu$ , **population variance** $\sigma^2$ 

i.e  $E[X]=\mu$                   $D[X]=E[(x-E[X])]^2=\sigma^2$

now we have the sample $x_1,x_2,\dots,x_n$ 

for each sample, $E[x_i] =\mu$ ,  $D[x_i]=\sigma^2$ 

sample mean $\overline{x}=\frac{1}{n}\sum_{i=1}^{n}x_i$  ; sample variance $S^2=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\overline{x})^2$ 

we want to see the bias between sample mean/variance and population mean/variance 

we can see the sample mean expectation:

$$
E[\overline{x}]=E[\frac{1}{n}\sum_{i=1}^{n}x_i]=\frac{1}{n}\sum_{i=1}^{n}E[x_i]=\frac{1}{n}\sum_{i=1}^{n}\mu=\mu
$$

**it is unbiased expectation** , and we can also get : 
$$
D[\overline{x}]=D[\frac{1}{n}\sum_{i=1}^{n}x_i]=\frac{1}{n^2}\sum_{i=1}^{n}D[x_i]=\frac{1}{n}\sigma^2
$$
we can see the sample variance expectation:

$$
\begin{aligned}
E[S^2]=E[\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\overline{x})^2]
&=\frac{1}{n-1}\sum_{i=1}^{n}E[(x_i-\overline{x})^2]\\
&=\frac{1}{n-1}\sum_{i=1}^{n}E\left(x_i^2-2x_i\overline{x} + \overline{x}^2\right)\\
&=\frac{1}{n-1}\left(\sum_{i=1}^{n}E[x_i^2]-2\sum_{i=1}^{n}E[x_i\overline{x}]+\sum_{i=1}^{n}E[\overline{x}^2] \right)\\
&=\frac{1}{n-1}\left(\sum_{i=1}^{n}E[x_i^2]
-2E[\overline{x}\sum_{i=1}^{n}x_i]
+nE[\overline{x}^2] \right)\\
&=\frac{1}{n-1}\left(\sum_{i=1}^{n}E[x_i^2]
-2E[\overline{x}n\overline{x}]
+nE[\overline{x}^2] \right)\\
&=\frac{1}{n-1}\left(\sum_{i=1}^{n}E[x_i^2]
-2nE[\overline{x}^2]
+nE[\overline{x}^2] \right)\\
&=\frac{1}{n-1}\left(\sum_{i=1}^{n}E[x_i^2]
-nE[\overline{x}^2] \right)\\
&=\frac{1}{n-1}\left(\sum_{i=1}^{n}(D[x_i]+{E[x_i]}^2)
-n(D[\overline{x}]+{E[\overline{x}]}^2) \right)\\
&=\frac{1}{n-1}\left(n(\sigma^2+\mu^2)
-n(\frac{1}{n}\sigma^2+\mu^2) \right)\\
&=\sigma^2
\end{aligned}
$$

**it is unbiased expectation** , so the denominator should be (n-1)

估计量的数学期望等于被估计参数的真实值，在多次重复下，它们的平均数接近所估计的参数真值 

Bias of an estimator :  is the difference between this estimator's expected value and the true value of the parameter being estimated. 






# 迹

相似矩阵的迹都相等



### 矩阵特征值之和等于矩阵的迹

矩阵A的特征方程如下：

$\operatorname{det}(\lambda I-A)=\left|\begin{array}{cccc}{\lambda-a_{11}} & {-a_{12}} & {\dots} & {-a_{1 n}} \\ {-a_{21}} & {\lambda-a_{22}} & {\dots} & {-a_{2 n}} \\ {\dots} & {\dots} & {\dots} & {\dots} \\ {-a_{n 1}} & {-a_{n 2}} & {\dots} & {\lambda-a_{n n}}\end{array}\right|$

行列式展开，如果想要 $\lambda^{n-1}$ 这一项，只有 $\left(\lambda-a_{11}\right)\left(\lambda-a_{22}\right) \ldots\left(\lambda-a_{n n}\right)$ ，

那么可以得到 $\lambda^{n-1}$ 的系数为 $-\left(a_{11}+a_{12}+\ldots+a_{n n}\right)$ ，

上面的特征方程又可以写为特征值的形式 $\operatorname{det}(\lambda I-A)=\left(\lambda-\lambda_{1}\right)\left(\lambda-\lambda_{2}\right) \ldots\left(\lambda-\lambda_{n}\right)$ ，

$\lambda^{n-1}$ 这一项的系数又恰好是 $-\left(\lambda_{1}+\lambda_{2}+\ldots+\lambda_{n}\right)$ 

所以 $\operatorname{tr}(A)=\sum_{k=1}^{n} \lambda_{k}$







## 补充

标量 $x$ 到向量 $\boldsymbol{x}$ 的变化

$x^2 \longrightarrow \boldsymbol{x}^T\boldsymbol{x}$ 

$ax \longrightarrow \boldsymbol{a}^T\boldsymbol{x}$ 

$w^Tw = \sum_j w_j^2$



## 参考

[Univariate Distribution Relationships](http://www.math.wm.edu/~leemis/2008amstat.pdf)

[https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)



